{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dcP0FJaNNbcVnA-c_TdB4bwFKcoc1QcZ","timestamp":1741319361670}],"cell_execution_strategy":"setup","private_outputs":true,"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyP4DO3LVpcRXye4D/x171LA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BEwHRsE-0vRR"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","#import tensorflow as tf\n","#from google.colab import drive    #google colab and drive linkage funtion\n"]},{"cell_type":"code","source":["!pip install datasets     #installing cos 1st public dataset gave me error for not having this.\n","!pip install transformers\n","!pip install torch"],"metadata":{"id":"KL7GogKKJq0L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##1. Excel file data loading"],"metadata":{"id":"8v3Gt0NdJFrU"}},{"cell_type":"code","source":["collected_data = pd.read_excel(\"/datacollection.xlsx\")"],"metadata":{"id":"Pz2ELOur2TN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data.shape[0]"],"metadata":{"id":"DdK77O_I4LZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data.head()"],"metadata":{"id":"aYjk1gjB4OOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data.columns"],"metadata":{"id":"yHwAx42K4Rey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data.dropna(axis=0)"],"metadata":{"id":"wOmd4PHs4djK","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data.shape[0]"],"metadata":{"id":"s-sduIW84uNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data.dropna(axis=0, inplace=True)"],"metadata":{"id":"bpAWWkHm44j9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data.shape[0]"],"metadata":{"id":"LSqt5HNP48Ou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data.columns"],"metadata":{"id":"1UfQo1W56yEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data_df = pd.DataFrame(collected_data)"],"metadata":{"id":"sIKUcxaU49Xh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data_df"],"metadata":{"collapsed":true,"id":"CpYYixE_5aO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# collected_data_df ['value']= collected_data_df.iloc[:, 0]\n","# collected_data_df ['label'] = collected_data_df.iloc[:, -1]\n"],"metadata":{"id":"NWgqeUhw5b--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data_df.shape[0]"],"metadata":{"id":"GGpj--Oy6637"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#collected_data_df.drop(['Incorrect Sentence', 'Corrected Sentence'],axis=1)\n","collected_data_df = collected_data_df.rename(columns={\"Incorrect Sentence\": \"value\", \"Corrected Sentence\": \"label\"})"],"metadata":{"id":"MDwU9Sn_72hf","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#collected_data_df.drop(['Incorrect Sentence', 'Corrected Sentence'],axis=1, inplace=True)"],"metadata":{"id":"h489dNJN8IyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data_df"],"metadata":{"id":"NkrtWiSt8TPB","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Regular Epression"],"metadata":{"id":"dRvIZEGEA2fp"}},{"cell_type":"code","source":["#testing regular expression to remove () at tail parts.\n","sample = '5. after dinner we went for a walk. (missing comma after \"dinner\")'\n","output = re.sub(r'\\(.*\\)', r'',sample)    #removing the whole (). we use \\( and \\), unless we do, python will think it's expression.\n","output = re.sub(r'^[0-9.]+', r'',output)     # removing numbers at the beginning of the sentences and . (eg. 2.)\n","output = re.sub(r'^[a-z]', r'[A-Z]', output)\n","print (output)"],"metadata":{"id":"SCXvuEwJ8VGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data_df.tail(20)"],"metadata":{"id":"zUTyoQ7N-LEr","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#removing the whole () from value column\n","collected_data_df['value'] = [re.sub(r'\\(.*\\)', r'',i) for i in collected_data_df['value']]"],"metadata":{"id":"zQtAmATl_8Ym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collected_data_df.tail(20)"],"metadata":{"id":"lFNC_hjsAhIu","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#removing the whole () from label column\n","#collected_data_df['label'] = [re.sub(r'\\(.*\\)', r'',i) for i in collected_data_df['label']]\n","#closing this as we don't have () in label"],"metadata":{"id":"xzAq6t63AjCx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2. Public Dataset loading"],"metadata":{"id":"iYrQU-7yJKKL"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","public_dataset = load_dataset(\"MohamedAshraf701/lang-8\")"],"metadata":{"id":"Hj_WuBnrJDVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print (public_dataset)"],"metadata":{"id":"TJc7VUglJNNt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" public_df = pd.DataFrame(public_dataset['train'])"],"metadata":{"id":"Ukqv1BGBLnSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["public_df.shape[0]"],"metadata":{"id":"cmwab-HoMJFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["public_df.columns"],"metadata":{"id":"xu5CEYPcMe8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["public_df.tail(10)"],"metadata":{"id":"fswR4T_kX4ME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#changing the name of column to the same as our own file. if we don't do that, when combined, there will be 4 columns.\n","public_df.rename(columns={'processed_input':'value', 'processed_output': 'label'},inplace=True)"],"metadata":{"id":"5HnlCtSGQ7w_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["public_df.head(10)"],"metadata":{"id":"fPub_jbvMi2U","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["public_df.tail(20)"],"metadata":{"id":"eAf_ME7rMleG","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["public_df.dropna(axis=0, inplace = True)"],"metadata":{"id":"LfCs3z4zNQ7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["public_df.shape[0]"],"metadata":{"id":"qbC6wskqNgtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#combining collected data and public data into one file.\n","\n","combined_data = pd.concat([collected_data_df, public_df], axis=0)"],"metadata":{"id":"hgdNoNwxN01e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_data.shape[0]"],"metadata":{"id":"gFs4Xj-nODOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_data.tail(20)"],"metadata":{"id":"8UCdbibiXewW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_data"],"metadata":{"id":"c7hNYtSsOExv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#removing extra white spaces\n","combined_data['value'] = [i.strip() for i in combined_data['value']]"],"metadata":{"id":"ZEt4LoPZdvSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_data['label'] = [i.strip() for i in combined_data['label']]"],"metadata":{"id":"N5gAp4Pkd4S_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#replacing with 1 space for location with more than 1 spaces.\n","combined_data['value'] = [re.sub(r\"\\s+\", \" \", i) for i in combined_data['value']]"],"metadata":{"id":"TSoffb2_eWHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_data['label'] = [re.sub(r\"\\s+\", \" \", i) for i in combined_data['label']]"],"metadata":{"id":"wRQaxxi0g5Ka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#capitalizing the first letter of the sentence\n","combined_data['value'] = [i.capitalize() for i in combined_data['value']]\n","combined_data['label'] = [i.capitalize() for i in combined_data['label']]"],"metadata":{"id":"u4I793JQhf6v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_data.head(20)"],"metadata":{"id":"vwmHTrXQd7Ks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_data.tail(20)"],"metadata":{"id":"2-W3xRh3hnnd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##splitting the data for training and testing"],"metadata":{"id":"NYgQGKrBUe-A"}},{"cell_type":"code","source":["#training data as 70%\n","combined_data_size = int(combined_data.shape[0]*0.7)\n","\n","#but which 70%? so assign first (head) 70% as training\n","train_combined_data = combined_data.head(combined_data_size)\n","\n","test_combined_data = combined_data.tail(combined_data.shape[0] - combined_data_size)"],"metadata":{"id":"6IRM8QSKMP4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_combined_data.shape[0]"],"metadata":{"id":"_NlGdWhbUePX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_combined_data.head(10)"],"metadata":{"id":"uHb2O5EEUkO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_combined_data.shape[0])\n","print(test_combined_data.shape[0])"],"metadata":{"id":"obV0be56kpAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_combined_data = train_combined_data.dropna(axis=0)"],"metadata":{"id":"1vDLKum-dTgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_combined_data = test_combined_data.dropna(axis=0)"],"metadata":{"id":"-DBACVmsdZXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_combined_data.shape[0]"],"metadata":{"id":"SWkTtJqLd9CY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_combined_data['value'] = 'grammar: ' + train_combined_data['value']\n","test_combined_data['value'] = 'grammar: ' + test_combined_data['value']"],"metadata":{"id":"Xg34ej6owR8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_combined_data.head(1)"],"metadata":{"id":"VKEVNy64wc7c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Modeling"],"metadata":{"id":"Fe52I9kEh5Ul"}},{"cell_type":"code","source":["#loading tokenizer, T5Tokenizer: To tokenize the dataset which consists of grammatically incorrect and correct sentences.\n","from transformers import T5Tokenizer\n","from datasets import Dataset"],"metadata":{"id":"jcOCi1DPh6PD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#https://debuggercafe.com/getting-started-with-grammar-correction/\n","model_name = 't5-small'\n","batch_size = 16   #Number of training samples processed at once (adjust based on GPU memory)\n","max_length = 128  #This is the maximum context length to consider for each sample in the JSON files. Beyond this length, the text samples will be truncated and smaller samples will be padded.\n","epochs = 3        #The number of epochs to train the model for.\n","num_workers = 2   #Number of CPU workers for data loading (lower this if using Colab/limited\n","out_dir = 'results_t5_small'  #This is the output directory to save intermediate results."],"metadata":{"id":"kE7ez2Gpum3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = T5Tokenizer.from_pretrained(model_name)"],"metadata":{"id":"1PxTOH3EjNg8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://wandb.ai/mukilan/T5_transformer/reports/Exploring-Google-s-T5-Text-To-Text-Transformer-Model--VmlldzoyNjkzOTE2#2.-training:\n","\n","\n","https://docs.python.org/3/library/re.html#search-vs-match"],"metadata":{"id":"vAhHrkVvs6C5"}},{"cell_type":"markdown","source":["##tokenizing the data"],"metadata":{"id":"_0hrEqkizSqt"}},{"cell_type":"code","source":["# #kept getting error. and it says bcos i didn't convert to dataset. so had to convert pd to huggingface dataset\n","# train_huggingface_dataset = Dataset.from_pandas(train_combined_data)\n","# test_huggingface_dataset = Dataset.from_pandas(test_combined_data)"],"metadata":{"id":"lgY9VyLGOW2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_huggingface_dataset = train_huggingface_dataset.filter(lambda example: example[\"label\"] is not None and example[\"label\"].strip() != \"\")\n","# test_huggingface_dataset = test_huggingface_dataset.filter(lambda example: example[\"label\"] is not None and example[\"label\"].strip() != \"\")"],"metadata":{"id":"JGXZHkd2pb_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_combined_data.loc[:,'value'] = 'grammar: ' + train_combined_data['value']\n","# test_combined_data.loc[:,'value'] = 'grammar: ' + test_combined_data['value']\n","\n","# #without .loc[:, I received below error\n","# # <ipython-input-42-0fdd2619f78b>:1: SettingWithCopyWarning:\n","# # A value is trying to be set on a copy of a slice from a DataFrame.\n","# # Try using .loc[row_indexer,col_indexer] = value instead\n","\n","# # See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","# #   train_combined_data['value'] = 'grammar: ' + train_combined_data['value']\n"],"metadata":{"id":"Qoe2HR49s6mQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##tokenizing the dataset"],"metadata":{"id":"66y8V-H02pdS"}},{"cell_type":"code","source":["# # Function to convert text data into model value and targets\n","# def preprocess_function(examples):\n","#     inputs = [f\"rectify: {inc}\" for inc in examples['value']]\n","#     model_inputs = tokenizer(\n","#         inputs,\n","#         max_length=max_length,\n","#         truncation=True,\n","#         padding='max_length'\n","#     )\n","#     # Set up the tokenizer for targets\n","#     with tokenizer.as_target_tokenizer():\n","#         labels = tokenizer(\n","#             examples['label'],\n","#             max_length=max_length,\n","#             truncation=True,\n","#             padding='max_length'\n","#         )\n","\n","#     #      # Convert padding tokens from 0 → -100 (so model ignores them)\n","#     # labels[\"input_ids\"] = [\n","#     #     [(l if l != 0 else -100) for l in label] for label in labels[\"input_ids\"]\n","#     # ]\n","\n","#     # model_inputs[\"labels\"] = labels[\"input_ids\"]\n","#     # return model_inputs\n","#     # Ensure labels exist and are properly formatted\n","\n","\n","#     if \"input_ids\" in labels:\n","#         model_inputs[\"labels\"] = labels[\"input_ids\"]\n","#     else:\n","#         model_inputs[\"labels\"] = [[-100] * max_length]  # Fill empty labels with ignored tokens\n","\n","#     return model_inputs\n"],"metadata":{"id":"DiqPwmEiMI4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Apply the function to the whole dataset\n","# tokenized_train = train_huggingface_dataset.map(\n","#     preprocess_function,\n","#     batched=True,\n","#     remove_columns=[\"value\", \"label\"]  # ✅ Prevents dataset conflicts\n","# )\n","#     # num_proc=8"],"metadata":{"id":"b1xyDwc4gw2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenized_test = test_huggingface_dataset.map(\n","#     preprocess_function,\n","#     batched=True,\n","#     remove_columns=[\"value\", \"label\"]  # ✅ Prevents dataset conflicts\n","\n","#   #  num_proc=8\n","# )"],"metadata":{"id":"UifqsefGVvYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_value = tokenizer(\n","        train_combined_data['value'].tolist(),\n","        max_length=max_length,\n","        truncation=True,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","test_value = tokenizer(\n","        test_combined_data['value'].tolist(),\n","        max_length=max_length,\n","        truncation=True,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","#https://huggingface.co/google-t5/t5-small\n","#https://debuggercafe.com/getting-started-with-grammar-correction/\n","\n","train_label = tokenizer(\n","        train_combined_data['label'].tolist(),\n","        max_length=max_length,\n","        truncation=True,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","test_label = tokenizer(\n","        test_combined_data['label'].tolist(),\n","        max_length=max_length,\n","        truncation=True,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","#https://stackoverflow.com/questions/75247437/valueerror-text-input-must-of-type-str-single-example\n","#https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"],"metadata":{"id":"TqnsGC8y0qBq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert tokenized input & labels to Hugging Face Dataset format\n","train_dataset = Dataset.from_dict({\n","    \"input_ids\": train_value[\"input_ids\"],\n","    \"attention_mask\": train_value[\"attention_mask\"],\n","    \"labels\": train_label[\"input_ids\"]\n","})\n","\n","# Convert tokenized input & labels to Hugging Face Dataset format\n","test_dataset = Dataset.from_dict({\n","    \"input_ids\": test_value[\"input_ids\"],\n","    \"attention_mask\": test_value[\"attention_mask\"],\n","    \"labels\": test_label[\"input_ids\"]\n","})\n","#https://huggingface.co/docs/datasets/en/use_datase"],"metadata":{"id":"QIZQUNyK3_SC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"wpNLwarb4yV1"}},{"cell_type":"markdown","source":["##loading model"],"metadata":{"id":"D0BFxXfj8sJP"}},{"cell_type":"code","source":["import torch\n","from transformers import T5ForConditionalGeneration,Trainer, TrainingArguments #This is for the loading of the T5 model."],"metadata":{"id":"2u25UyCl4wbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the pre-trained model\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","# Resize token embeddings (IMPORTANT: Add this here)\n","model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"mFWQvPwk8q8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify the device (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"DV5eOuY39aP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Total parameters and trainable parameters.\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"{total_params:,} total parameters.\")\n","total_trainable_params = sum(\n","    p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params:,} training parameters.\")"],"metadata":{"id":"mRQ6XNFw9bhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir=out_dir,\n","    num_train_epochs=epochs,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size*2,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir=out_dir,\n","    eval_strategy='epoch',\n","    save_strategy='epoch',\n","    #save_steps=500,\n","    #eval_steps=500,\n","    load_best_model_at_end=True,\n","    save_total_limit=2,\n","    report_to='tensorboard',\n","    dataloader_num_workers=num_workers,\n","    fp16=True,\n",")"],"metadata":{"id":"lIyGdAOoEnx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")\n","\n","# Start training\n","history = trainer.train()"],"metadata":{"id":"CIufAcC424Zi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save model & tokenizer after training\n","model.save_pretrained(out_dir)  # Use your defined output directory\n","tokenizer.save_pretrained(out_dir)\n","\n","print(f\"✅ Model and tokenizer saved to {out_dir}\")"],"metadata":{"id":"yP2YET_rDs_7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"uC4RKiLi1td0"}},{"cell_type":"code","source":[],"metadata":{"id":"Fdl9ZCWqaPgi"},"execution_count":null,"outputs":[]}]}