{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "private_outputs": true,
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNIo8xzOc/1g5KVepfotKcI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayyMem/ProjectGG/blob/main/Copy_of_GGProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEwHRsE-0vRR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "#import tensorflow as tf\n",
        "#from google.colab import drive    #google colab and drive linkage funtion\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets     #installing cos 1st public dataset gave me error for not having this.\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "KL7GogKKJq0L",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Excel file data loading"
      ],
      "metadata": {
        "id": "8v3Gt0NdJFrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data = pd.read_excel(\"/datacollection.xlsx\")"
      ],
      "metadata": {
        "id": "Pz2ELOur2TN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data.shape[0]"
      ],
      "metadata": {
        "id": "DdK77O_I4LZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data.head()"
      ],
      "metadata": {
        "id": "aYjk1gjB4OOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data.columns"
      ],
      "metadata": {
        "id": "yHwAx42K4Rey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data.dropna(axis=0)"
      ],
      "metadata": {
        "id": "wOmd4PHs4djK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data.shape[0]"
      ],
      "metadata": {
        "id": "s-sduIW84uNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "bpAWWkHm44j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data.shape[0]"
      ],
      "metadata": {
        "id": "LSqt5HNP48Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data.columns"
      ],
      "metadata": {
        "id": "1UfQo1W56yEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data_df = pd.DataFrame(collected_data)"
      ],
      "metadata": {
        "id": "sIKUcxaU49Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CpYYixE_5aO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collected_data_df ['value']= collected_data_df.iloc[:, 0]\n",
        "# collected_data_df ['label'] = collected_data_df.iloc[:, -1]\n"
      ],
      "metadata": {
        "id": "NWgqeUhw5b--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data_df.shape[0]"
      ],
      "metadata": {
        "id": "GGpj--Oy6637"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collected_data_df.drop(['Incorrect Sentence', 'Corrected Sentence'],axis=1)\n",
        "collected_data_df = collected_data_df.rename(columns={\"Incorrect Sentence\": \"value\", \"Corrected Sentence\": \"label\"})"
      ],
      "metadata": {
        "id": "MDwU9Sn_72hf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collected_data_df.drop(['Incorrect Sentence', 'Corrected Sentence'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "h489dNJN8IyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data_df"
      ],
      "metadata": {
        "id": "NkrtWiSt8TPB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regular Epression"
      ],
      "metadata": {
        "id": "dRvIZEGEA2fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing regular expression to remove () at tail parts.\n",
        "sample = '5. after dinner we went for a walk. (missing comma after \"dinner\")'\n",
        "output = re.sub(r'\\(.*\\)', r'',sample)    #removing the whole (). we use \\( and \\), unless we do, python will think it's expression.\n",
        "output = re.sub(r'^[0-9.]+', r'',output)     # removing numbers at the beginning of the sentences and . (eg. 2.)\n",
        "output = re.sub(r'^[a-z]', r'[A-Z]', output)\n",
        "print (output)"
      ],
      "metadata": {
        "id": "SCXvuEwJ8VGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data_df.tail(20)"
      ],
      "metadata": {
        "id": "zUTyoQ7N-LEr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the whole () from value column\n",
        "collected_data_df['value'] = [re.sub(r'\\(.*\\)', r'',i) for i in collected_data_df['value']]"
      ],
      "metadata": {
        "id": "zQtAmATl_8Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data_df.tail(20)"
      ],
      "metadata": {
        "id": "lFNC_hjsAhIu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the whole () from label column\n",
        "#collected_data_df['label'] = [re.sub(r'\\(.*\\)', r'',i) for i in collected_data_df['label']]\n",
        "#closing this as we don't have () in label"
      ],
      "metadata": {
        "id": "xzAq6t63AjCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Public Dataset loading"
      ],
      "metadata": {
        "id": "iYrQU-7yJKKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "public_dataset = load_dataset(\"MohamedAshraf701/lang-8\")"
      ],
      "metadata": {
        "id": "Hj_WuBnrJDVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (public_dataset)"
      ],
      "metadata": {
        "id": "TJc7VUglJNNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " public_df = pd.DataFrame(public_dataset['train'])"
      ],
      "metadata": {
        "id": "Ukqv1BGBLnSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_df.shape[0]"
      ],
      "metadata": {
        "id": "cmwab-HoMJFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_df.columns"
      ],
      "metadata": {
        "id": "xu5CEYPcMe8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_df.tail(10)"
      ],
      "metadata": {
        "id": "fswR4T_kX4ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing the name of column to the same as our own file. if we don't do that, when combined, there will be 4 columns.\n",
        "public_df.rename(columns={'processed_input':'value', 'processed_output': 'label'},inplace=True)"
      ],
      "metadata": {
        "id": "5HnlCtSGQ7w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_df.head(10)"
      ],
      "metadata": {
        "id": "fPub_jbvMi2U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_df.tail(20)"
      ],
      "metadata": {
        "id": "eAf_ME7rMleG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_df.dropna(axis=0, inplace = True)"
      ],
      "metadata": {
        "id": "LfCs3z4zNQ7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_df.shape[0]"
      ],
      "metadata": {
        "id": "qbC6wskqNgtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining collected data and public data into one file.\n",
        "\n",
        "combined_data = pd.concat([collected_data_df, public_df], axis=0)"
      ],
      "metadata": {
        "id": "hgdNoNwxN01e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.shape[0]"
      ],
      "metadata": {
        "id": "gFs4Xj-nODOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.tail(20)"
      ],
      "metadata": {
        "id": "8UCdbibiXewW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data"
      ],
      "metadata": {
        "id": "c7hNYtSsOExv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing extra white spaces\n",
        "combined_data['value'] = [i.strip() for i in combined_data['value']]"
      ],
      "metadata": {
        "id": "ZEt4LoPZdvSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data['label'] = [i.strip() for i in combined_data['label']]"
      ],
      "metadata": {
        "id": "N5gAp4Pkd4S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing with 1 space for location with more than 1 spaces.\n",
        "combined_data['value'] = [re.sub(r\"\\s+\", \" \", i) for i in combined_data['value']]"
      ],
      "metadata": {
        "id": "TSoffb2_eWHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data['label'] = [re.sub(r\"\\s+\", \" \", i) for i in combined_data['label']]"
      ],
      "metadata": {
        "id": "wRQaxxi0g5Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#capitalizing the first letter of the sentence\n",
        "combined_data['value'] = [i.capitalize() for i in combined_data['value']]\n",
        "combined_data['label'] = [i.capitalize() for i in combined_data['label']]"
      ],
      "metadata": {
        "id": "u4I793JQhf6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.head(20)"
      ],
      "metadata": {
        "id": "vwmHTrXQd7Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.tail(20)"
      ],
      "metadata": {
        "id": "2-W3xRh3hnnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spelling local dataloading"
      ],
      "metadata": {
        "id": "S9lSFvLjZjvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/bittlingmayer/spelling/data?select=wikipedia.txt"
      ],
      "metadata": {
        "id": "FWT_VCMDZnzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_data = pd.read_csv(\"/spelling.csv\")"
      ],
      "metadata": {
        "id": "qhEhdc66Zi_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_data.columns"
      ],
      "metadata": {
        "id": "VJr7nV-x1O14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_df = pd.DataFrame(spelling_data)"
      ],
      "metadata": {
        "id": "IAsaoBxW1yuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_df = spelling_df.rename(columns={\"Wrong\": \"value\", \"Correct\": \"label\"})"
      ],
      "metadata": {
        "id": "D_3z0Njp2CCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_df.columns"
      ],
      "metadata": {
        "id": "pyb951-s2B8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_df['value'] = \"The word '\" + spelling_df['value'] + \"' is spelled incorrectly.\"\n",
        "spelling_df['label'] = \"Correct spelling of the word is '\" + spelling_df['label'] + \"'.\""
      ],
      "metadata": {
        "id": "K3C55dGW2WWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spelling_df['value'] = spelling_df['value'].str.replace('The word ', \"\", regex=False)\n",
        "# spelling_df['label'] = spelling_df['label'].str.replace('Correct spelling of the word is ', \"\", regex=False)"
      ],
      "metadata": {
        "id": "f43LeCzy3MZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_df.tail(2)"
      ],
      "metadata": {
        "id": "I2jSgMxQ5pN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_df.head(10)"
      ],
      "metadata": {
        "id": "g_j7lXMe12E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##splitting the data for training and testing"
      ],
      "metadata": {
        "id": "NYgQGKrBUe-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training data as 70%\n",
        "combined_data_size = int(combined_data.shape[0]*0.7)\n",
        "\n",
        "#but which 70%? so assign first (head) 70% as training\n",
        "train_combined_data = combined_data.head(combined_data_size)\n",
        "\n",
        "test_combined_data = combined_data.tail(combined_data.shape[0] - combined_data_size)"
      ],
      "metadata": {
        "id": "54TiRkXt4L8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data['value'] = 'grammar: ' + train_combined_data['value']\n",
        "test_combined_data['value'] = 'grammar: ' + test_combined_data['value']"
      ],
      "metadata": {
        "id": "Xg34ej6owR8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data.shape[0]"
      ],
      "metadata": {
        "id": "asOIUmpy__Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_combined_data.shape[0]"
      ],
      "metadata": {
        "id": "7whG2WaWBSST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training data as 70%\n",
        "spelling_df_data_size = int(spelling_df.shape[0]*0.7)\n",
        "\n",
        "#but which 70%? so assign first (head) 70% as training\n",
        "train_spelling_df = spelling_df.head(spelling_df_data_size)\n",
        "\n",
        "test_spelling_df = spelling_df.tail(spelling_df.shape[0] - spelling_df_data_size)"
      ],
      "metadata": {
        "id": "6IRM8QSKMP4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spelling_df.shape[0]"
      ],
      "metadata": {
        "id": "WrDkbhbeBTXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_spelling_df.shape[0]"
      ],
      "metadata": {
        "id": "0KuDeiqwBW5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_spelling_df = test_spelling_df.dropna(axis=0)"
      ],
      "metadata": {
        "id": "XSOZtZnU47hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spelling_df = train_spelling_df.dropna(axis=0)"
      ],
      "metadata": {
        "id": "xE5NB0984-3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spelling_df.shape[0]"
      ],
      "metadata": {
        "id": "7v_jDvHZ4lZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_spelling_df.shape[0]"
      ],
      "metadata": {
        "id": "nJX7Nay0Bh9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data.head(10)"
      ],
      "metadata": {
        "id": "JMtiE-DABkKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spelling_df.head(10)"
      ],
      "metadata": {
        "id": "FPZtSFiRBm0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_spelling_df.tail(20)"
      ],
      "metadata": {
        "id": "qIFGdZMP4pil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data.shape[0]"
      ],
      "metadata": {
        "id": "_NlGdWhbUePX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data.head(10)"
      ],
      "metadata": {
        "id": "uHb2O5EEUkO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_combined_data.shape[0])\n",
        "print(test_combined_data.shape[0])"
      ],
      "metadata": {
        "id": "obV0be56kpAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data = pd.concat([train_combined_data, train_spelling_df], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "CmJgFppnCMgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data = train_combined_data.dropna(axis=0)"
      ],
      "metadata": {
        "id": "1vDLKum-dTgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_combined_data = pd.concat([test_combined_data, test_spelling_df], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "BDQ10kqoCjSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_combined_data = test_combined_data.dropna(axis=0)"
      ],
      "metadata": {
        "id": "-DBACVmsdZXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data.shape[0]"
      ],
      "metadata": {
        "id": "SWkTtJqLd9CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_combined_data.shape[0]"
      ],
      "metadata": {
        "id": "xOXZZIrbC5qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data.shape[0]"
      ],
      "metadata": {
        "id": "VKEVNy64wc7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data.head(10)"
      ],
      "metadata": {
        "id": "98rJjV-DDrdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_data.tail(10)"
      ],
      "metadata": {
        "id": "F2P6CI3qDtoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_combined_data.head(10)"
      ],
      "metadata": {
        "id": "BPhV27BsDwki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_combined_data.tail(10)"
      ],
      "metadata": {
        "id": "5Go4w0SGD2Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total training samples after adding spelling data: {test_combined_data.shape[0]}\")"
      ],
      "metadata": {
        "id": "F9cfh3y_D-J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_combined_data.isnull().sum())"
      ],
      "metadata": {
        "id": "_4Y5hJdGFdDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modeling"
      ],
      "metadata": {
        "id": "Fe52I9kEh5Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading tokenizer, T5Tokenizer: To tokenize the dataset which consists of grammatically incorrect and correct sentences.\n",
        "from transformers import T5Tokenizer\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "jcOCi1DPh6PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://debuggercafe.com/getting-started-with-grammar-correction/\n",
        "model_name = 't5-small'\n",
        "batch_size = 16   #Number of training samples processed at once (adjust based on GPU memory)\n",
        "max_length = 128  #This is the maximum context length to consider for each sample in the JSON files. Beyond this length, the text samples will be truncated and smaller samples will be padded.\n",
        "epochs = 3        #The number of epochs to train the model for.\n",
        "num_workers = 2   #Number of CPU workers for data loading (lower this if using Colab/limited\n",
        "out_dir = 'results_t5_small'  #This is the output directory to save intermediate results."
      ],
      "metadata": {
        "id": "kE7ez2Gpum3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "1PxTOH3EjNg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wandb.ai/mukilan/T5_transformer/reports/Exploring-Google-s-T5-Text-To-Text-Transformer-Model--VmlldzoyNjkzOTE2#2.-training:\n",
        "\n",
        "\n",
        "https://docs.python.org/3/library/re.html#search-vs-match"
      ],
      "metadata": {
        "id": "vAhHrkVvs6C5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokenizing the data"
      ],
      "metadata": {
        "id": "_0hrEqkizSqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_combined_data.loc[:,'value'] = 'grammar: ' + train_combined_data['value']\n",
        "# test_combined_data.loc[:,'value'] = 'grammar: ' + test_combined_data['value']\n",
        "\n",
        "# #without .loc[:, I received below error\n",
        "# # <ipython-input-42-0fdd2619f78b>:1: SettingWithCopyWarning:\n",
        "# # A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "# # Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "# # See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "# #   train_combined_data['value'] = 'grammar: ' + train_combined_data['value']\n"
      ],
      "metadata": {
        "id": "Qoe2HR49s6mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokenizing the dataset"
      ],
      "metadata": {
        "id": "66y8V-H02pdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_value = tokenizer(\n",
        "        train_combined_data['value'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "train_label = tokenizer(\n",
        "        train_combined_data['label'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "test_value = tokenizer(\n",
        "        test_combined_data['value'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "#https://huggingface.co/google-t5/t5-small\n",
        "#https://debuggercafe.com/getting-started-with-grammar-correction/\n",
        "\n",
        "test_label = tokenizer(\n",
        "        test_combined_data['label'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "#https://stackoverflow.com/questions/75247437/valueerror-text-input-must-of-type-str-single-example\n",
        "#https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "\n",
        "print(train_value[\"input_ids\"].shape)  # Ensure shape is correct\n",
        "print(train_label[\"input_ids\"].shape)"
      ],
      "metadata": {
        "id": "TqnsGC8y0qBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokenized input & labels to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": train_value[\"input_ids\"],\n",
        "    \"attention_mask\": train_value[\"attention_mask\"],\n",
        "    \"labels\": train_label[\"input_ids\"]\n",
        "})\n",
        "\n",
        "# Convert tokenized input & labels to Hugging Face Dataset format\n",
        "test_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": test_value[\"input_ids\"],\n",
        "    \"attention_mask\": test_value[\"attention_mask\"],\n",
        "    \"labels\": test_label[\"input_ids\"]\n",
        "})\n",
        "#https://huggingface.co/docs/datasets/en/use_datase"
      ],
      "metadata": {
        "id": "QIZQUNyK3_SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wpNLwarb4yV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##loading model"
      ],
      "metadata": {
        "id": "D0BFxXfj8sJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,Trainer, TrainingArguments #This is for the loading of the T5 model."
      ],
      "metadata": {
        "id": "2u25UyCl4wbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Resize token embeddings (IMPORTANT: Add this here)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "mFWQvPwk8q8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "DV5eOuY39aP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total parameters and trainable parameters.\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")"
      ],
      "metadata": {
        "id": "mRQ6XNFw9bhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=out_dir,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=out_dir,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    #save_steps=500,\n",
        "    #eval_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    report_to='tensorboard',\n",
        "    dataloader_num_workers=num_workers,\n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "id": "lIyGdAOoEnx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "history = trainer.train()"
      ],
      "metadata": {
        "id": "CIufAcC424Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model & tokenizer after training\n",
        "model.save_pretrained(out_dir)  # Use your defined output directory\n",
        "tokenizer.save_pretrained(out_dir)\n",
        "\n",
        "print(f\"✅ Model and tokenizer saved to {out_dir}\")"
      ],
      "metadata": {
        "id": "yP2YET_rDs_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "# Define the correct model path\n",
        "model_path = \"results_t5_small\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load the model (SafeTensors format is automatically handled)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "Qmq2T82BCYgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uC4RKiLi1td0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_args.max_steps)"
      ],
      "metadata": {
        "id": "eRrVxNwY7OKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_value[\"input_ids\"]))\n",
        "print(len(train_label[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "Fdl9ZCWqaPgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_data.shape[0])  # Should be 602,000\n",
        "print(train_combined_data.shape[0])  # Should be ~70% of combined_data"
      ],
      "metadata": {
        "id": "8d6BnpaO7VJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "OOOxVXzQ7Xdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "IS4TvWrPFOrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "IeDbr914FTzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(out_dir))  # This should list model files"
      ],
      "metadata": {
        "id": "GV3cZLXiBTgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_path = \"results_t5_small\"  # Make sure this is the correct folder where your model is saved\n",
        "# Upload to Hugging Face\n",
        "model.push_to_hub(\"TayyMem/Grammar_Spelling_Correction\")\n",
        "tokenizer.push_to_hub(\"TayyMem/Grammar_Spelling_Correction\")\n",
        "\n",
        "print(\"✅ Model uploaded successfully to Hugging Face!\")\n"
      ],
      "metadata": {
        "id": "Tb-y6E0tFUM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "example_sentence = [\n",
        "    \"He is there when I had finished having dinner\",\n",
        "    \"She go to school everyday.\",\n",
        "    \"The clothes he wear is given by me.\",\n",
        "    \"he mistaked that it were me.\",\n",
        "    \"I comitted to this job and i planned it perfect\",\n",
        "    \"This are the luxerious clothes I have ever want.\"\n",
        "]\n",
        "\n",
        "#tokenizing the example sentence\n",
        "testing_input = tokenizer(\n",
        "    example_sentence,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_tensors='pt').to(device)\n",
        "\n",
        "#need to read why this one below.\n",
        "testing_output = model.generate(\n",
        "    input_ids= testing_input[\"input_ids\"],\n",
        "    attention_mask= testing_input[\"attention_mask\"],\n",
        "    max_length=max_length) # A way to predict data\n",
        "\n",
        "corrected_sentences = [tokenizer.decode(output, skip_special_tokens=True) for output in testing_output]\n",
        "\n",
        "for i in range(len(example_sentence)):\n",
        "    print(f\"❌ Incorrect: {example_sentence[i]}\")\n",
        "    print(f\"✅ Corrected: {corrected_sentences[i]}\\n\")\n",
        "\n",
        "# print('Accuracy Score',accuracy_score(test_label,corrected_sentences)*100,'%')\n",
        "\n",
        "# print('Precision Macro Score ',precision_score(test_label,corrected_sentences,average = 'macro')*100,'%')\n",
        "\n",
        "# print('Recall_Score',recall_score(test_label,corrected_sentences, average = 'macro')*100,'%')\n",
        "\n",
        "# print('F1_Score',f1_score(test_label,corrected_sentences, average = 'macro')*100,'%')\n",
        "\n"
      ],
      "metadata": {
        "id": "wGa6Zz2jMrdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"Their is some misstake in this sentance.\",\n",
        "    \"She go to school everyday.\",\n",
        "    \"I has a pen.\",\n",
        "]"
      ],
      "metadata": {
        "id": "pshdJLF4NrSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}